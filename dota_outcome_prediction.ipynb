{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awWnrs3xX0Wc",
        "outputId": "56af3b8f-a2e5-4565-82ea-803a54060836"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Training Accuracy: 0.6849925120201782\n",
            "Test Accuracy: 0.6590479192938209\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.66      0.63      0.65      3132\n",
            "        True       0.66      0.69      0.67      3212\n",
            "\n",
            "    accuracy                           0.66      6344\n",
            "   macro avg       0.66      0.66      0.66      6344\n",
            "weighted avg       0.66      0.66      0.66      6344\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.sparse import csr_matrix\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load data from Google Drive\n",
        "match_data = pd.read_csv('/content/drive/MyDrive/match_data.csv')\n",
        "immortal_vs_all_winrates = pd.read_csv('/content/drive/MyDrive/immortal_vs_all_winrates.csv')\n",
        "immortal_with_all_winrates = pd.read_csv('/content/drive/MyDrive/immortal_with_all_winrates.csv')\n",
        "\n",
        "# Filter matches with duration < 15 minutes and remove matches with leavers\n",
        "match_data = match_data[(match_data['durationSeconds'] >= 900) &\n",
        "                        (match_data['leaverStatus'] == 'NONE')]\n",
        "\n",
        "# Initialize lists for storing features\n",
        "radiant_features_sparse = []\n",
        "dire_features_sparse = []\n",
        "match_index = {}\n",
        "synergy_features = []\n",
        "counter_synergy_features = []\n",
        "\n",
        "# Iterate through each match\n",
        "for idx, (match_id, group) in enumerate(match_data.groupby('match_id')):\n",
        "    match_index[match_id] = idx\n",
        "\n",
        "\n",
        "    radiant_heroes = np.zeros(138)\n",
        "    dire_heroes = np.zeros(138)\n",
        "\n",
        "    # Separate Radiant and Dire heroes\n",
        "    for _, row in group.iterrows():\n",
        "        if row['isRadiant']:\n",
        "            radiant_heroes[row['heroId'] - 1] = 1\n",
        "        else:\n",
        "            dire_heroes[row['heroId'] - 1] = 1\n",
        "\n",
        "\n",
        "    radiant_features_sparse.append(radiant_heroes)\n",
        "    dire_features_sparse.append(dire_heroes)\n",
        "\n",
        "    # Calculate synergy within Radiant and Dire teams\n",
        "    radiant_synergy = 0\n",
        "    dire_synergy = 0\n",
        "    radiant_hero_ids = group[group['isRadiant']]['heroId'].values\n",
        "    dire_hero_ids = group[~group['isRadiant']]['heroId'].values\n",
        "\n",
        "    for i in range(len(radiant_hero_ids)):\n",
        "        for j in range(i + 1, len(radiant_hero_ids)):\n",
        "            synergy = immortal_with_all_winrates[(immortal_with_all_winrates['HeroId1'] == radiant_hero_ids[i]) &\n",
        "                                                 (immortal_with_all_winrates['HeroId2'] == radiant_hero_ids[j])]['WinRate']\n",
        "            if not synergy.empty:\n",
        "                radiant_synergy += synergy.values[0]\n",
        "\n",
        "    for i in range(len(dire_hero_ids)):\n",
        "        for j in range(i + 1, len(dire_hero_ids)):\n",
        "            synergy = immortal_with_all_winrates[(immortal_with_all_winrates['HeroId1'] == dire_hero_ids[i]) &\n",
        "                                                 (immortal_with_all_winrates['HeroId2'] == dire_hero_ids[j])]['WinRate']\n",
        "            if not synergy.empty:\n",
        "                dire_synergy += synergy.values[0]\n",
        "\n",
        "    # Calculate counter synergy between Radiant and Dire teams\n",
        "    counter_synergy = 0\n",
        "    for radiant_hero in radiant_hero_ids:\n",
        "        for dire_hero in dire_hero_ids:\n",
        "            counter = immortal_vs_all_winrates[(immortal_vs_all_winrates['HeroId1'] == radiant_hero) &\n",
        "                                               (immortal_vs_all_winrates['HeroId2'] == dire_hero)]['WinRate']\n",
        "            if not counter.empty:\n",
        "                counter_synergy += counter.values[0]\n",
        "\n",
        "    synergy_features.append(radiant_synergy - dire_synergy)\n",
        "    counter_synergy_features.append(counter_synergy)\n",
        "\n",
        "\n",
        "radiant_df = pd.DataFrame(radiant_features_sparse, columns=[f'radiant_hero_{i}' for i in range(1, 139)])\n",
        "dire_df = pd.DataFrame(dire_features_sparse, columns=[f'dire_hero_{i}' for i in range(1, 139)])\n",
        "synergy_df = pd.DataFrame({\n",
        "    'match_id': list(match_index.keys()),\n",
        "    'synergy': synergy_features,\n",
        "    'counter_synergy': counter_synergy_features\n",
        "})\n",
        "\n",
        "final_df = pd.concat([radiant_df, dire_df], axis=1)\n",
        "final_df['match_id'] = list(match_index.keys())\n",
        "final_df = final_df.merge(synergy_df, on='match_id')\n",
        "\n",
        "\n",
        "target = match_data.groupby('match_id').first()['didRadiantWin'].reset_index()\n",
        "final_df = final_df.merge(target[['match_id', 'didRadiantWin']], on='match_id')\n",
        "\n",
        "\n",
        "X = final_df.drop(columns=['didRadiantWin', 'match_id'])\n",
        "y = final_df['didRadiantWin']\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a logistic regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_train = model.predict(X_train)\n",
        "y_pred_test = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
        "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
        "\n",
        "print(f'Training Accuracy: {train_accuracy}')\n",
        "print(f'Test Accuracy: {test_accuracy}')\n",
        "print(classification_report(y_test, y_pred_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "my7NgUOb5iLt",
        "outputId": "e4b801e2-8899-44e7-c004-b8c372e83a5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-840e627eb3d6>\u001b[0m in \u001b[0;36m<cell line: 29>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mradiant_hero\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mradiant_hero_ids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdire_hero\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdire_hero_ids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             counter = immortal_vs_all_winrates[(immortal_vs_all_winrates['HeroId1'] == radiant_hero) &\n\u001b[0m\u001b[1;32m     72\u001b[0m                                                (immortal_vs_all_winrates['HeroId2'] == dire_hero)]['WinRate']\n\u001b[1;32m     73\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcounter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3882\u001b[0m         \u001b[0;31m# Do we have a (boolean) 1d indexer?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3883\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_bool_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3884\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_bool_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3885\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3886\u001b[0m         \u001b[0;31m# We are left with two options: a single key, and a collection of keys,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_bool_array\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3945\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3946\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take_with_is_copy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3948\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_take_with_is_copy\u001b[0;34m(self, indices, axis)\u001b[0m\n\u001b[1;32m   4086\u001b[0m         \u001b[0mSee\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdocstring\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfull\u001b[0m \u001b[0mexplanation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4087\u001b[0m         \"\"\"\n\u001b[0;32m-> 4088\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4089\u001b[0m         \u001b[0;31m# Maybe set copy if we didn't actually change the index.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4090\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indices, axis, **kwargs)\u001b[0m\n\u001b[1;32m   4066\u001b[0m             )\n\u001b[1;32m   4067\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4068\u001b[0;31m         new_data = self._mgr.take(\n\u001b[0m\u001b[1;32m   4069\u001b[0m             \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4070\u001b[0m             \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_block_manager_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indexer, axis, verify)\u001b[0m\n\u001b[1;32m    872\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_convert_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m         \u001b[0mnew_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexers/utils.py\u001b[0m in \u001b[0;36mmaybe_convert_indices\u001b[0;34m(indices, n, verify)\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"indices are out-of-bounds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_any\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_prod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0;31m# Parsing keyword arguments is currently fairly slow, so avoid it for now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwhere\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.sparse import csr_matrix\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedShuffleSplit\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load data from Google Drive\n",
        "match_data = pd.read_csv('/content/drive/MyDrive/match_data.csv')\n",
        "immortal_vs_all_winrates = pd.read_csv('/content/drive/MyDrive/immortal_vs_all_winrates.csv')\n",
        "immortal_with_all_winrates = pd.read_csv('/content/drive/MyDrive/immortal_with_all_winrates.csv')\n",
        "\n",
        "# Filter matches with duration < 15 minutes and remove matches with leavers\n",
        "match_data = match_data[(match_data['durationSeconds'] >= 900) &\n",
        "                        (match_data['leaverStatus'] == 'NONE')]\n",
        "\n",
        "# Initialize lists for storing features\n",
        "radiant_features_sparse = []\n",
        "dire_features_sparse = []\n",
        "match_index = {}\n",
        "synergy_features = []\n",
        "counter_synergy_features = []\n",
        "\n",
        "# Iterate through each match\n",
        "for idx, (match_id, group) in enumerate(match_data.groupby('match_id')):\n",
        "    match_index[match_id] = idx\n",
        "\n",
        "    # Initialize feature vectors for Radiant and Dire teams\n",
        "    radiant_heroes = np.zeros(138)\n",
        "    dire_heroes = np.zeros(138)\n",
        "\n",
        "    # Separate Radiant and Dire heroes\n",
        "    for _, row in group.iterrows():\n",
        "        if row['isRadiant']:\n",
        "            radiant_heroes[row['heroId'] - 1] = 1  # HeroId adjusted for 0-based indexing\n",
        "        else:\n",
        "            dire_heroes[row['heroId'] - 1] = 1  # HeroId adjusted for 0-based indexing\n",
        "\n",
        "    # Append the features to the list\n",
        "    radiant_features_sparse.append(radiant_heroes)\n",
        "    dire_features_sparse.append(dire_heroes)\n",
        "\n",
        "    # Calculate synergy within Radiant and Dire teams\n",
        "    radiant_synergy = 0\n",
        "    dire_synergy = 0\n",
        "    radiant_hero_ids = group[group['isRadiant']]['heroId'].values\n",
        "    dire_hero_ids = group[~group['isRadiant']]['heroId'].values\n",
        "\n",
        "    for i in range(len(radiant_hero_ids)):\n",
        "        for j in range(i + 1, len(radiant_hero_ids)):\n",
        "            synergy = immortal_with_all_winrates[(immortal_with_all_winrates['HeroId1'] == radiant_hero_ids[i]) &\n",
        "                                                 (immortal_with_all_winrates['HeroId2'] == radiant_hero_ids[j])]['WinRate']\n",
        "            if not synergy.empty:\n",
        "                radiant_synergy += synergy.values[0]\n",
        "\n",
        "    for i in range(len(dire_hero_ids)):\n",
        "        for j in range(i + 1, len(dire_hero_ids)):\n",
        "            synergy = immortal_with_all_winrates[(immortal_with_all_winrates['HeroId1'] == dire_hero_ids[i]) &\n",
        "                                                 (immortal_with_all_winrates['HeroId2'] == dire_hero_ids[j])]['WinRate']\n",
        "            if not synergy.empty:\n",
        "                dire_synergy += synergy.values[0]\n",
        "\n",
        "    # Calculate counter synergy between Radiant and Dire teams\n",
        "    counter_synergy = 0\n",
        "    for radiant_hero in radiant_hero_ids:\n",
        "        for dire_hero in dire_hero_ids:\n",
        "            counter = immortal_vs_all_winrates[(immortal_vs_all_winrates['HeroId1'] == radiant_hero) &\n",
        "                                               (immortal_vs_all_winrates['HeroId2'] == dire_hero)]['WinRate']\n",
        "            if not counter.empty:\n",
        "                counter_synergy += counter.values[0]\n",
        "\n",
        "    synergy_features.append(radiant_synergy - dire_synergy)  # Net synergy\n",
        "    counter_synergy_features.append(counter_synergy)\n",
        "\n",
        "# Convert hero features and synergy features to DataFrames\n",
        "radiant_df = pd.DataFrame(radiant_features_sparse, columns=[f'radiant_hero_{i}' for i in range(1, 139)])\n",
        "dire_df = pd.DataFrame(dire_features_sparse, columns=[f'dire_hero_{i}' for i in range(1, 139)])\n",
        "synergy_df = pd.DataFrame({\n",
        "    'match_id': list(match_index.keys()),\n",
        "    'synergy': synergy_features,\n",
        "    'counter_synergy': counter_synergy_features\n",
        "})\n",
        "\n",
        "# Combine Radiant and Dire features with synergy features\n",
        "final_df = pd.concat([radiant_df, dire_df], axis=1)\n",
        "final_df['match_id'] = list(match_index.keys())\n",
        "final_df = final_df.merge(synergy_df, on='match_id')\n",
        "\n",
        "# Add the target variable\n",
        "target = match_data.groupby('match_id').first()['didRadiantWin'].reset_index()\n",
        "final_df = final_df.merge(target[['match_id', 'didRadiantWin']], on='match_id')\n",
        "\n",
        "# Separate features and target variable\n",
        "X = final_df.drop(columns=['didRadiantWin', 'match_id'])\n",
        "y = final_df['didRadiantWin']\n",
        "\n",
        "# Define the model\n",
        "model = LogisticRegression(penalty='elasticnet', solver='saga', max_iter=1000)\n",
        "\n",
        "# Define the hyperparameter grid, including train-test split ratios\n",
        "param_grid = {\n",
        "    'C': [  0.1, 1.0, 10.0 ],  # Broad range of C values\n",
        "    'l1_ratio': [ 0.25, 0.5, 1.0],   # Different L1 ratio values\n",
        "    'split_ratio': [ 0.6, 0.65, 0.7]        # Train-test split ratios\n",
        "}\n",
        "\n",
        "# Custom train-test split function to be used in GridSearchCV\n",
        "class CustomTrainTestSplit:\n",
        "    def __init__(self, split_ratio):\n",
        "        self.split_ratio = split_ratio\n",
        "\n",
        "    def split(self, X, y):\n",
        "        sss = StratifiedShuffleSplit(n_splits=1, test_size=1-self.split_ratio, random_state=42)\n",
        "        for train_index, test_index in sss.split(X, y):\n",
        "            yield train_index, test_index\n",
        "\n",
        "# Extend GridSearchCV to include the split ratio\n",
        "def grid_search_with_split_ratio(X, y, param_grid, model):\n",
        "    best_score = -np.inf\n",
        "    best_params = None\n",
        "    best_model = None\n",
        "    best_split_ratio = None  # Track the best split ratio\n",
        "\n",
        "    for split_ratio in param_grid['split_ratio']:\n",
        "        splitter = CustomTrainTestSplit(split_ratio)\n",
        "        for train_index, test_index in splitter.split(X, y):\n",
        "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "            grid_search = GridSearchCV(estimator=model,\n",
        "                                       param_grid={k: v for k, v in param_grid.items() if k != 'split_ratio'},\n",
        "                                       cv=5,\n",
        "                                       scoring='accuracy')\n",
        "            grid_search.fit(X_train, y_train)\n",
        "\n",
        "            if grid_search.best_score_ > best_score:\n",
        "                best_score = grid_search.best_score_\n",
        "                best_params = grid_search.best_params_\n",
        "                best_model = grid_search.best_estimator_\n",
        "                best_split_ratio = split_ratio  # Store the best split ratio\n",
        "\n",
        "    return best_model, best_params, best_score, best_split_ratio\n",
        "\n",
        "# Perform the grid search including train-test split ratios\n",
        "best_model, best_params, best_score, best_split_ratio = grid_search_with_split_ratio(X, y, param_grid, model)\n",
        "\n",
        "print(f\"Best Parameters: {best_params}\")\n",
        "print(f\"Best Cross-Validation Accuracy: {best_score}\")\n",
        "print(f\"Best Train-Test Split Ratio: {best_split_ratio}\")  # Print the best split ratio\n",
        "\n",
        "# Evaluate the best model on the test data\n",
        "y_pred_test = best_model.predict(X_test)\n",
        "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_test)}\")\n",
        "print(classification_report(y_test, y_pred_test))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AD-Ql8kiBq_H",
        "outputId": "a883b3c3-5263-43f6-98a1-0e49db308d3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.6635741307872456\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.66      0.62      0.64      5362\n",
            "        True       0.67      0.70      0.68      5740\n",
            "\n",
            "    accuracy                           0.66     11102\n",
            "   macro avg       0.66      0.66      0.66     11102\n",
            "weighted avg       0.66      0.66      0.66     11102\n",
            "\n",
            "Model saved successfully.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import joblib\n",
        "\n",
        "# Assuming you've already done the preprocessing and have X and y\n",
        "# Initialize the model with the best parameters\n",
        "final_model = LogisticRegression(penalty='elasticnet', solver='saga', max_iter=1000, C=1.0, l1_ratio=1.0)\n",
        "\n",
        "# Use the best Train-Test Split Ratio\n",
        "split_ratio = 0.65\n",
        "sss = StratifiedShuffleSplit(n_splits=1, test_size=1-split_ratio, random_state=42)\n",
        "\n",
        "for train_index, test_index in sss.split(X, y):\n",
        "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "# Train the final model on the training data\n",
        "final_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the final model on the test data\n",
        "y_pred_test = final_model.predict(X_test)\n",
        "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_test)}\")\n",
        "print(classification_report(y_test, y_pred_test))\n",
        "\n",
        "# Save the model using joblib\n",
        "joblib.dump(final_model, '/content/drive/MyDrive/final_logistic_model.pkl')\n",
        "\n",
        "print(\"Model saved successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLUw9aFvD2qE",
        "outputId": "fe0e01cd-35e1-4ac0-80b5-4a76b05305f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction (1 = Radiant win, 0 = Dire win): True\n",
            "Probability of Radiant win: 0.72\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:465: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:465: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Assuming `model` is your trained model and you have loaded any required data (e.g., synergy data)\n",
        "\n",
        "def prepare_input_vector(radiant_hero_ids, dire_hero_ids, immortal_with_all_winrates, immortal_vs_all_winrates):\n",
        "    #  Initialize empty hero vectors\n",
        "    radiant_heroes = np.zeros(138)\n",
        "    dire_heroes = np.zeros(138)\n",
        "\n",
        "    #  Update vectors based on input hero IDs\n",
        "    for hero_id in radiant_hero_ids:\n",
        "        radiant_heroes[hero_id - 1] = 1  # -1 for 0-based indexing\n",
        "\n",
        "    for hero_id in dire_hero_ids:\n",
        "        dire_heroes[hero_id - 1] = 1  # -1 for 0-based indexing\n",
        "\n",
        "    #  Calculate synergy and counter synergy\n",
        "    radiant_synergy = 0\n",
        "    dire_synergy = 0\n",
        "    counter_synergy = 0\n",
        "\n",
        "    # Calculate Radiant synergy\n",
        "    for i in range(len(radiant_hero_ids)):\n",
        "        for j in range(i + 1, len(radiant_hero_ids)):\n",
        "            synergy = immortal_with_all_winrates[(immortal_with_all_winrates['HeroId1'] == radiant_hero_ids[i]) &\n",
        "                                                 (immortal_with_all_winrates['HeroId2'] == radiant_hero_ids[j])]['WinRate']\n",
        "            if not synergy.empty:\n",
        "                radiant_synergy += synergy.values[0]\n",
        "\n",
        "    # Calculate Dire synergy\n",
        "    for i in range(len(dire_hero_ids)):\n",
        "        for j in range(i + 1, len(dire_hero_ids)):\n",
        "            synergy = immortal_with_all_winrates[(immortal_with_all_winrates['HeroId1'] == dire_hero_ids[i]) &\n",
        "                                                 (immortal_with_all_winrates['HeroId2'] == dire_hero_ids[j])]['WinRate']\n",
        "            if not synergy.empty:\n",
        "                dire_synergy += synergy.values[0]\n",
        "\n",
        "    # Calculate counter synergy\n",
        "    for radiant_hero in radiant_hero_ids:\n",
        "        for dire_hero in dire_hero_ids:\n",
        "            counter = immortal_vs_all_winrates[(immortal_vs_all_winrates['HeroId1'] == radiant_hero) &\n",
        "                                               (immortal_vs_all_winrates['HeroId2'] == dire_hero)]['WinRate']\n",
        "            if not counter.empty:\n",
        "                counter_synergy += counter.values[0]\n",
        "\n",
        "    # Net synergy\n",
        "    net_synergy = radiant_synergy - dire_synergy\n",
        "\n",
        "    #  Combine all features into a single input vector\n",
        "    input_vector = np.concatenate((radiant_heroes, dire_heroes, [net_synergy, counter_synergy]))\n",
        "\n",
        "    return input_vector\n",
        "\n",
        "# Example usage:\n",
        "radiant_hero_ids = [1, 2, 3, 4, 5]  # Example Radiant hero IDs input by the user\n",
        "dire_hero_ids = [6, 7, 8, 9, 10]    # Example Dire hero IDs input by the user\n",
        "\n",
        "# Prepare the input vector\n",
        "input_vector = prepare_input_vector(radiant_hero_ids, dire_hero_ids, immortal_with_all_winrates, immortal_vs_all_winrates)\n",
        "\n",
        "# Reshape the input vector to match the model's expected input shape\n",
        "input_vector = input_vector.reshape(1, -1)\n",
        "\n",
        "# Step 5: Predict the outcome using the trained model\n",
        "prediction = model.predict(input_vector)\n",
        "predicted_probability = model.predict_proba(input_vector)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Prediction (1 = Radiant win, 0 = Dire win): {prediction[0]}\")\n",
        "print(f\"Probability of Radiant win: {predicted_probability[0][1]:.2f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMOpT54nFnjs",
        "outputId": "ef9b1588-ec11-4c62-ffcc-797653a39131"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Test Accuracy: 0.6569987389659521\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.67      0.57      0.62      5362\n",
            "        True       0.65      0.74      0.69      5740\n",
            "\n",
            "    accuracy                           0.66     11102\n",
            "   macro avg       0.66      0.65      0.65     11102\n",
            "weighted avg       0.66      0.66      0.65     11102\n",
            "\n",
            "Missing synergy pairings: 0 pairs\n",
            "Missing counter-synergy pairings: 0 pairs\n",
            "Model saved to dota2_model.pkl\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.sparse import csr_matrix\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import joblib\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load data from Google Drive\n",
        "match_data = pd.read_csv('/content/drive/MyDrive/match_data.csv')\n",
        "immortal_vs_all_winrates = pd.read_csv('/content/drive/MyDrive/immortal_vs_all_winrates.csv')\n",
        "immortal_with_all_winrates = pd.read_csv('/content/drive/MyDrive/immortal_with_all_winrates.csv')\n",
        "\n",
        "# Filter matches with duration < 15 minutes and remove matches with leavers\n",
        "match_data = match_data[(match_data['durationSeconds'] >= 900) &\n",
        "                        (match_data['leaverStatus'] == 'NONE')]\n",
        "\n",
        "# Initialize lists for storing features\n",
        "radiant_features_sparse = []\n",
        "dire_features_sparse = []\n",
        "match_index = {}\n",
        "synergy_features = []\n",
        "counter_synergy_features = []\n",
        "\n",
        "# Initialize counters for missing pairings\n",
        "missing_synergy_pairings = []\n",
        "missing_counter_synergy_pairings = []\n",
        "\n",
        "# Iterate through each match\n",
        "for idx, (match_id, group) in enumerate(match_data.groupby('match_id')):\n",
        "    match_index[match_id] = idx\n",
        "\n",
        "    # Initialize feature vectors for Radiant and Dire teams\n",
        "    radiant_heroes = np.zeros(138)\n",
        "    dire_heroes = np.zeros(138)\n",
        "\n",
        "    # Separate Radiant and Dire heroes\n",
        "    for _, row in group.iterrows():\n",
        "        if row['isRadiant']:\n",
        "            radiant_heroes[row['heroId'] - 1] = 1  # HeroId adjusted for 0-based indexing\n",
        "        else:\n",
        "            dire_heroes[row['heroId'] - 1] = 1  # HeroId adjusted for 0-based indexing\n",
        "\n",
        "    # Append the features to the list\n",
        "    radiant_features_sparse.append(radiant_heroes)\n",
        "    dire_features_sparse.append(dire_heroes)\n",
        "\n",
        "    # Calculate synergy within Radiant and Dire teams\n",
        "    radiant_synergy = 0\n",
        "    dire_synergy = 0\n",
        "    radiant_hero_ids = group[group['isRadiant']]['heroId'].values\n",
        "    dire_hero_ids = group[~group['isRadiant']]['heroId'].values\n",
        "\n",
        "    for i in range(len(radiant_hero_ids)):\n",
        "        for j in range(i + 1, len(radiant_hero_ids)):\n",
        "            synergy = immortal_with_all_winrates[\n",
        "                (immortal_with_all_winrates['HeroId1'] == radiant_hero_ids[i]) &\n",
        "                (immortal_with_all_winrates['HeroId2'] == radiant_hero_ids[j])\n",
        "            ]['WinRate']\n",
        "\n",
        "            if synergy.empty:\n",
        "                # Check the reversed pairing\n",
        "                synergy = immortal_with_all_winrates[\n",
        "                    (immortal_with_all_winrates['HeroId1'] == radiant_hero_ids[j]) &\n",
        "                    (immortal_with_all_winrates['HeroId2'] == radiant_hero_ids[i])\n",
        "                ]['WinRate']\n",
        "\n",
        "            if synergy.empty:\n",
        "                missing_synergy_pairings.append((radiant_hero_ids[i], radiant_hero_ids[j]))\n",
        "            else:\n",
        "                radiant_synergy += synergy.values[0]\n",
        "\n",
        "    for i in range(len(dire_hero_ids)):\n",
        "        for j in range(i + 1, len(dire_hero_ids)):\n",
        "            synergy = immortal_with_all_winrates[\n",
        "                (immortal_with_all_winrates['HeroId1'] == dire_hero_ids[i]) &\n",
        "                (immortal_with_all_winrates['HeroId2'] == dire_hero_ids[j])\n",
        "            ]['WinRate']\n",
        "\n",
        "            if synergy.empty:\n",
        "                # Check the reversed pairing\n",
        "                synergy = immortal_with_all_winrates[\n",
        "                    (immortal_with_all_winrates['HeroId1'] == dire_hero_ids[j]) &\n",
        "                    (immortal_with_all_winrates['HeroId2'] == dire_hero_ids[i])\n",
        "                ]['WinRate']\n",
        "\n",
        "            if synergy.empty:\n",
        "                missing_synergy_pairings.append((dire_hero_ids[i], dire_hero_ids[j]))\n",
        "            else:\n",
        "                dire_synergy += synergy.values[0]\n",
        "\n",
        "    # Calculate counter synergy between Radiant and Dire teams\n",
        "    counter_synergy = 0\n",
        "    for radiant_hero in radiant_hero_ids:\n",
        "        for dire_hero in dire_hero_ids:\n",
        "            counter = immortal_vs_all_winrates[\n",
        "                (immortal_vs_all_winrates['HeroId1'] == radiant_hero) &\n",
        "                (immortal_vs_all_winrates['HeroId2'] == dire_hero)\n",
        "            ]['WinRate']\n",
        "\n",
        "            if counter.empty:\n",
        "                # Check the reversed pairing\n",
        "                counter = immortal_vs_all_winrates[\n",
        "                    (immortal_vs_all_winrates['HeroId1'] == dire_hero) &\n",
        "                    (immortal_vs_all_winrates['HeroId2'] == radiant_hero)\n",
        "                ]['WinRate']\n",
        "\n",
        "                if not counter.empty:\n",
        "                    counter_synergy += 100 - counter.values[0]  # Complement the win rate\n",
        "\n",
        "            if counter.empty:\n",
        "                missing_counter_synergy_pairings.append((radiant_hero, dire_hero))\n",
        "            else:\n",
        "                counter_synergy += counter.values[0]\n",
        "\n",
        "    synergy_features.append(radiant_synergy - dire_synergy)  # Net synergy\n",
        "    counter_synergy_features.append(counter_synergy)\n",
        "\n",
        "\n",
        "# Convert hero features and synergy features to DataFrames\n",
        "radiant_df = pd.DataFrame(radiant_features_sparse, columns=[f'radiant_hero_{i}' for i in range(1, 139)])\n",
        "dire_df = pd.DataFrame(dire_features_sparse, columns=[f'dire_hero_{i}' for i in range(1, 139)])\n",
        "synergy_df = pd.DataFrame({\n",
        "    'match_id': list(match_index.keys()),\n",
        "    'synergy': synergy_features,\n",
        "    'counter_synergy': counter_synergy_features\n",
        "})\n",
        "\n",
        "# Combine Radiant and Dire features with synergy features\n",
        "final_df = pd.concat([radiant_df, dire_df], axis=1)\n",
        "final_df['match_id'] = list(match_index.keys())\n",
        "final_df = final_df.merge(synergy_df, on='match_id')\n",
        "\n",
        "# Add the target variable\n",
        "target = match_data.groupby('match_id').first()['didRadiantWin'].reset_index()\n",
        "final_df = final_df.merge(target[['match_id', 'didRadiantWin']], on='match_id')\n",
        "\n",
        "# Separate features and target variable\n",
        "X = final_df.drop(columns=['didRadiantWin', 'match_id'])\n",
        "y = final_df['didRadiantWin']\n",
        "\n",
        "# Define the model with the best parameters\n",
        "model = LogisticRegression(penalty='elasticnet', solver='saga', max_iter=1000, C=1.0, l1_ratio=1.0)\n",
        "\n",
        "# Best split ratio from the grid search\n",
        "split_ratio = 0.65\n",
        "\n",
        "# Split the data using the best split ratio\n",
        "sss = StratifiedShuffleSplit(n_splits=1, test_size=1-split_ratio, random_state=42)\n",
        "for train_index, test_index in sss.split(X, y):\n",
        "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "# Train the model on the best split\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "y_pred_test = model.predict(X_test)\n",
        "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_test)}\")\n",
        "print(classification_report(y_test, y_pred_test))\n",
        "# At the end of your processing, you can print or log the missing pairings\n",
        "print(f\"Missing synergy pairings: {len(missing_synergy_pairings)} pairs\")\n",
        "print(f\"Missing counter-synergy pairings: {len(missing_counter_synergy_pairings)} pairs\")\n",
        "# Save the trained model\n",
        "model_filename = 'dota2_model.pkl'\n",
        "joblib.dump(model,'/content/drive/MyDrive/dota2_mode.pkl')\n",
        "\n",
        "print(f\"Model saved to {model_filename}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "vrLkHfbKSSby",
        "outputId": "99a006fe-e1c5-4da5-c87c-d4c75816dc63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "list indices must be integers or slices, not str",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-7ac05067e3e8>\u001b[0m in \u001b[0;36m<cell line: 188>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;31m# Perform the grid search including train-test split ratios\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m \u001b[0mbest_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_search_with_split_ratio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Best Parameters: {best_params}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-7ac05067e3e8>\u001b[0m in \u001b[0;36mgrid_search_with_split_ratio\u001b[0;34m(X, y, param_grid, model)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0msplit_ratio\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'split_ratio'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m         \u001b[0msplitter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomTrainTestSplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_ratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtrain_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msplitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.sparse import csr_matrix\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedShuffleSplit\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load data from Google Drive\n",
        "match_data = pd.read_csv('/content/drive/MyDrive/match_data.csv')\n",
        "immortal_vs_all_winrates = pd.read_csv('/content/drive/MyDrive/immortal_vs_all_winrates.csv')\n",
        "immortal_with_all_winrates = pd.read_csv('/content/drive/MyDrive/immortal_with_all_winrates.csv')\n",
        "\n",
        "# Filter matches with duration < 15 minutes and remove matches with leavers\n",
        "match_data = match_data[(match_data['durationSeconds'] >= 900) &\n",
        "                        (match_data['leaverStatus'] == 'NONE')]\n",
        "\n",
        "# Initialize lists for storing features\n",
        "radiant_features_sparse = []\n",
        "dire_features_sparse = []\n",
        "match_index = {}\n",
        "synergy_features = []\n",
        "counter_synergy_features = []\n",
        "\n",
        "# Initialize counters for missing pairings\n",
        "missing_synergy_pairings = []\n",
        "missing_counter_synergy_pairings = []\n",
        "\n",
        "# Iterate through each match\n",
        "for idx, (match_id, group) in enumerate(match_data.groupby('match_id')):\n",
        "    match_index[match_id] = idx\n",
        "\n",
        "    # Initialize feature vectors for Radiant and Dire teams\n",
        "    radiant_heroes = np.zeros(138)\n",
        "    dire_heroes = np.zeros(138)\n",
        "\n",
        "    # Separate Radiant and Dire heroes\n",
        "    for _, row in group.iterrows():\n",
        "        if row['isRadiant']:\n",
        "            radiant_heroes[row['heroId'] - 1] = 1  # HeroId adjusted for 0-based indexing\n",
        "        else:\n",
        "            dire_heroes[row['heroId'] - 1] = 1  # HeroId adjusted for 0-based indexing\n",
        "\n",
        "    # Append the features to the list\n",
        "    radiant_features_sparse.append(radiant_heroes)\n",
        "    dire_features_sparse.append(dire_heroes)\n",
        "\n",
        "    # Calculate synergy within Radiant and Dire teams\n",
        "    radiant_synergy = 0\n",
        "    dire_synergy = 0\n",
        "    radiant_hero_ids = group[group['isRadiant']]['heroId'].values\n",
        "    dire_hero_ids = group[~group['isRadiant']]['heroId'].values\n",
        "\n",
        "    for i in range(len(radiant_hero_ids)):\n",
        "        for j in range(i + 1, len(radiant_hero_ids)):\n",
        "            synergy = immortal_with_all_winrates[\n",
        "                (immortal_with_all_winrates['HeroId1'] == radiant_hero_ids[i]) &\n",
        "                (immortal_with_all_winrates['HeroId2'] == radiant_hero_ids[j])\n",
        "            ]['WinRate']\n",
        "\n",
        "            if synergy.empty:\n",
        "                # Check the reversed pairing\n",
        "                synergy = immortal_with_all_winrates[\n",
        "                    (immortal_with_all_winrates['HeroId1'] == radiant_hero_ids[j]) &\n",
        "                    (immortal_with_all_winrates['HeroId2'] == radiant_hero_ids[i])\n",
        "                ]['WinRate']\n",
        "\n",
        "            if synergy.empty:\n",
        "                missing_synergy_pairings.append((radiant_hero_ids[i], radiant_hero_ids[j]))\n",
        "            else:\n",
        "                radiant_synergy += synergy.values[0]\n",
        "\n",
        "    for i in range(len(dire_hero_ids)):\n",
        "        for j in range(i + 1, len(dire_hero_ids)):\n",
        "            synergy = immortal_with_all_winrates[\n",
        "                (immortal_with_all_winrates['HeroId1'] == dire_hero_ids[i]) &\n",
        "                (immortal_with_all_winrates['HeroId2'] == dire_hero_ids[j])\n",
        "            ]['WinRate']\n",
        "\n",
        "            if synergy.empty:\n",
        "                # Check the reversed pairing\n",
        "                synergy = immortal_with_all_winrates[\n",
        "                    (immortal_with_all_winrates['HeroId1'] == dire_hero_ids[j]) &\n",
        "                    (immortal_with_all_winrates['HeroId2'] == dire_hero_ids[i])\n",
        "                ]['WinRate']\n",
        "\n",
        "            if synergy.empty:\n",
        "                missing_synergy_pairings.append((dire_hero_ids[i], dire_hero_ids[j]))\n",
        "            else:\n",
        "                dire_synergy += synergy.values[0]\n",
        "\n",
        "    # Calculate counter synergy between Radiant and Dire teams\n",
        "    counter_synergy = 0\n",
        "    for radiant_hero in radiant_hero_ids:\n",
        "        for dire_hero in dire_hero_ids:\n",
        "            counter = immortal_vs_all_winrates[\n",
        "                (immortal_vs_all_winrates['HeroId1'] == radiant_hero) &\n",
        "                (immortal_vs_all_winrates['HeroId2'] == dire_hero)\n",
        "            ]['WinRate']\n",
        "\n",
        "            if counter.empty:\n",
        "                # Check the reversed pairing\n",
        "                counter = immortal_vs_all_winrates[\n",
        "                    (immortal_vs_all_winrates['HeroId1'] == dire_hero) &\n",
        "                    (immortal_vs_all_winrates['HeroId2'] == radiant_hero)\n",
        "                ]['WinRate']\n",
        "\n",
        "                if not counter.empty:\n",
        "                    counter_synergy += 100 - counter.values[0]  # Complement the win rate\n",
        "\n",
        "            if counter.empty:\n",
        "                missing_counter_synergy_pairings.append((radiant_hero, dire_hero))\n",
        "            else:\n",
        "                counter_synergy += counter.values[0]\n",
        "\n",
        "    synergy_features.append(radiant_synergy - dire_synergy)  # Net synergy\n",
        "    counter_synergy_features.append(counter_synergy)\n",
        "\n",
        "# Convert hero features and synergy features to DataFrames\n",
        "radiant_df = pd.DataFrame(radiant_features_sparse, columns=[f'radiant_hero_{i}' for i in range(1, 139)])\n",
        "dire_df = pd.DataFrame(dire_features_sparse, columns=[f'dire_hero_{i}' for i in range(1, 139)])\n",
        "synergy_df = pd.DataFrame({\n",
        "    'match_id': list(match_index.keys()),\n",
        "    'synergy': synergy_features,\n",
        "    'counter_synergy': counter_synergy_features\n",
        "})\n",
        "\n",
        "# Combine Radiant and Dire features with synergy features\n",
        "final_df = pd.concat([radiant_df, dire_df], axis=1)\n",
        "final_df['match_id'] = list(match_index.keys())\n",
        "final_df = final_df.merge(synergy_df, on='match_id')\n",
        "\n",
        "# Add the target variable\n",
        "target = match_data.groupby('match_id').first()['didRadiantWin'].reset_index()\n",
        "final_df = final_df.merge(target[['match_id', 'didRadiantWin']], on='match_id')\n",
        "\n",
        "# Separate features and target variable\n",
        "X = final_df.drop(columns=['didRadiantWin', 'match_id'])\n",
        "y = final_df['didRadiantWin']\n",
        "\n",
        "# Define the model\n",
        "model = LogisticRegression(penalty='elasticnet', solver='saga', max_iter=1000)\n",
        "\n",
        "# Define the hyperparameter grid, including train-test split ratios\n",
        "param_grid = [\n",
        "  {'C': [1, 10, 100, 1000], 'kernel': ['linear'], 'gamma': [0.001, 0.0001], }\n",
        " ]\n",
        "\n",
        "# Custom train-test split function to be used in GridSearchCV\n",
        "class CustomTrainTestSplit:\n",
        "    def __init__(self, split_ratio):\n",
        "        self.split_ratio = split_ratio\n",
        "\n",
        "    def split(self, X, y):\n",
        "        sss = StratifiedShuffleSplit(n_splits=1, test_size=1-self.split_ratio, random_state=42)\n",
        "        for train_index, test_index in sss.split(X, y):\n",
        "            yield train_index, test_index\n",
        "\n",
        "# Extend GridSearchCV to include the split ratio\n",
        "def grid_search_with_split_ratio(X, y, param_grid, model):\n",
        "    best_score = -np.inf\n",
        "    best_params = None\n",
        "    best_model = None\n",
        "\n",
        "    for split_ratio in param_grid['split_ratio']:\n",
        "        splitter = CustomTrainTestSplit(split_ratio)\n",
        "        for train_index, test_index in splitter.split(X, y):\n",
        "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "            grid_search = GridSearchCV(estimator=model,\n",
        "                                       param_grid={k: v for k, v in param_grid.items() if k != 'split_ratio'},\n",
        "                                       cv=5,\n",
        "                                       scoring='accuracy')\n",
        "            grid_search.fit(X_train, y_train)\n",
        "\n",
        "            if grid_search.best_score_ > best_score:\n",
        "                best_score = grid_search.best_score_\n",
        "                best_params = grid_search.best_params_\n",
        "                best_model = grid_search.best_estimator_\n",
        "\n",
        "    return best_model, best_params, best_score\n",
        "\n",
        "# Perform the grid search including train-test split ratios\n",
        "best_model, best_params, best_score = grid_search_with_split_ratio(X, y, param_grid, model)\n",
        "\n",
        "print(f\"Best Parameters: {best_params}\")\n",
        "print(f\"Best Cross-Validation Accuracy: {best_score}\")\n",
        "\n",
        "# Evaluate the best model on the test data\n",
        "y_pred_test = best_model.predict(X_test)\n",
        "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_test)}\")\n",
        "print(classification_report(y_test, y_pred_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HqZg9Rdb7-UH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43cadd32-1cb9-4d5d-b9d6-5524f9a0853d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Training Accuracy: 0.6922293364377182\n",
            "Test Accuracy: 0.6612322104125383\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.66      0.63      0.65      5420\n",
            "        True       0.66      0.69      0.68      5682\n",
            "\n",
            "    accuracy                           0.66     11102\n",
            "   macro avg       0.66      0.66      0.66     11102\n",
            "weighted avg       0.66      0.66      0.66     11102\n",
            "\n",
            "Model saved to lastmodel.pkl\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.sparse import csr_matrix\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import joblib\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load data from Google Drive\n",
        "match_data = pd.read_csv('/content/drive/MyDrive/match_data.csv')\n",
        "immortal_vs_all_winrates = pd.read_csv('/content/drive/MyDrive/immortal_vs_all_winrates.csv')\n",
        "immortal_with_all_winrates = pd.read_csv('/content/drive/MyDrive/immortal_with_all_winrates.csv')\n",
        "\n",
        "# Filter matches with duration < 15 minutes and remove matches with leavers\n",
        "match_data = match_data[(match_data['durationSeconds'] >= 900) &\n",
        "                        (match_data['leaverStatus'] == 'NONE')]\n",
        "\n",
        "# Initialize lists for storing features\n",
        "radiant_features_sparse = []\n",
        "dire_features_sparse = []\n",
        "match_index = {}\n",
        "synergy_features = []\n",
        "counter_synergy_features = []\n",
        "\n",
        "# Initialize counters for missing pairings\n",
        "missing_synergy_pairings = []\n",
        "missing_counter_synergy_pairings = []\n",
        "\n",
        "# Iterate through each match\n",
        "for idx, (match_id, group) in enumerate(match_data.groupby('match_id')):\n",
        "    match_index[match_id] = idx\n",
        "\n",
        "    # Initialize feature vectors for Radiant and Dire teams\n",
        "    radiant_heroes = np.zeros(138)\n",
        "    dire_heroes = np.zeros(138)\n",
        "\n",
        "    # Separate Radiant and Dire heroes\n",
        "    for _, row in group.iterrows():\n",
        "        if row['isRadiant']:\n",
        "            radiant_heroes[row['heroId'] - 1] = 1  # HeroId adjusted for 0-based indexing\n",
        "        else:\n",
        "            dire_heroes[row['heroId'] - 1] = 1  # HeroId adjusted for 0-based indexing\n",
        "\n",
        "    # Append the features to the list\n",
        "    radiant_features_sparse.append(radiant_heroes)\n",
        "    dire_features_sparse.append(dire_heroes)\n",
        "\n",
        "    # Calculate synergy within Radiant and Dire teams\n",
        "    radiant_synergy = 0\n",
        "    dire_synergy = 0\n",
        "    radiant_hero_ids = group[group['isRadiant']]['heroId'].values\n",
        "    dire_hero_ids = group[~group['isRadiant']]['heroId'].values\n",
        "\n",
        "    for i in range(len(radiant_hero_ids)):\n",
        "        for j in range(i + 1, len(radiant_hero_ids)):\n",
        "            synergy = immortal_with_all_winrates[\n",
        "                (immortal_with_all_winrates['HeroId1'] == radiant_hero_ids[i]) &\n",
        "                (immortal_with_all_winrates['HeroId2'] == radiant_hero_ids[j])\n",
        "            ]['WinRate']\n",
        "\n",
        "            if synergy.empty:\n",
        "                # Check the reversed pairing\n",
        "                synergy = immortal_with_all_winrates[\n",
        "                    (immortal_with_all_winrates['HeroId1'] == radiant_hero_ids[j]) &\n",
        "                    (immortal_with_all_winrates['HeroId2'] == radiant_hero_ids[i])\n",
        "                ]['WinRate']\n",
        "\n",
        "            if synergy.empty:\n",
        "                missing_synergy_pairings.append((radiant_hero_ids[i], radiant_hero_ids[j]))\n",
        "            else:\n",
        "                radiant_synergy += synergy.values[0]\n",
        "\n",
        "    for i in range(len(dire_hero_ids)):\n",
        "        for j in range(i + 1, len(dire_hero_ids)):\n",
        "            synergy = immortal_with_all_winrates[\n",
        "                (immortal_with_all_winrates['HeroId1'] == dire_hero_ids[i]) &\n",
        "                (immortal_with_all_winrates['HeroId2'] == dire_hero_ids[j])\n",
        "            ]['WinRate']\n",
        "\n",
        "            if synergy.empty:\n",
        "                # Check the reversed pairing\n",
        "                synergy = immortal_with_all_winrates[\n",
        "                    (immortal_with_all_winrates['HeroId1'] == dire_hero_ids[j]) &\n",
        "                    (immortal_with_all_winrates['HeroId2'] == dire_hero_ids[i])\n",
        "                ]['WinRate']\n",
        "\n",
        "            if synergy.empty:\n",
        "                missing_synergy_pairings.append((dire_hero_ids[i], dire_hero_ids[j]))\n",
        "            else:\n",
        "                dire_synergy += synergy.values[0]\n",
        "\n",
        "    # Calculate counter synergy between Radiant and Dire teams\n",
        "    counter_synergy = 0\n",
        "    for radiant_hero in radiant_hero_ids:\n",
        "        for dire_hero in dire_hero_ids:\n",
        "            counter = immortal_vs_all_winrates[\n",
        "                (immortal_vs_all_winrates['HeroId1'] == radiant_hero) &\n",
        "                (immortal_vs_all_winrates['HeroId2'] == dire_hero)\n",
        "            ]['WinRate']\n",
        "\n",
        "            if counter.empty:\n",
        "                # Check the reversed pairing\n",
        "                counter = immortal_vs_all_winrates[\n",
        "                    (immortal_vs_all_winrates['HeroId1'] == dire_hero) &\n",
        "                    (immortal_vs_all_winrates['HeroId2'] == radiant_hero)\n",
        "                ]['WinRate']\n",
        "\n",
        "                if not counter.empty:\n",
        "                    counter_synergy += 100 - counter.values[0]\n",
        "\n",
        "            if counter.empty:\n",
        "                missing_counter_synergy_pairings.append((radiant_hero, dire_hero))\n",
        "            else:\n",
        "                counter_synergy += counter.values[0]\n",
        "\n",
        "    synergy_features.append(radiant_synergy - dire_synergy)\n",
        "    counter_synergy_features.append(counter_synergy)\n",
        "\n",
        "# Convert hero features and synergy features to DataFrames\n",
        "radiant_df = pd.DataFrame(radiant_features_sparse, columns=[f'radiant_hero_{i}' for i in range(1, 139)])\n",
        "dire_df = pd.DataFrame(dire_features_sparse, columns=[f'dire_hero_{i}' for i in range(1, 139)])\n",
        "synergy_df = pd.DataFrame({\n",
        "    'match_id': list(match_index.keys()),\n",
        "    'synergy': synergy_features,\n",
        "    'counter_synergy': counter_synergy_features\n",
        "})\n",
        "\n",
        "# Combine Radiant and Dire features with synergy features\n",
        "final_df = pd.concat([radiant_df, dire_df], axis=1)\n",
        "final_df['match_id'] = list(match_index.keys())\n",
        "final_df = final_df.merge(synergy_df, on='match_id')\n",
        "\n",
        "# Add the target variable\n",
        "target = match_data.groupby('match_id').first()['didRadiantWin'].reset_index()\n",
        "final_df = final_df.merge(target[['match_id', 'didRadiantWin']], on='match_id')\n",
        "\n",
        "# Separate features and target variable\n",
        "X = final_df.drop(columns=['didRadiantWin', 'match_id'])\n",
        "y = final_df['didRadiantWin']\n",
        "\n",
        "# Standardize the synergy and counter-synergy features\n",
        "scaler = StandardScaler()\n",
        "X[['synergy', 'counter_synergy']] = scaler.fit_transform(X[['synergy', 'counter_synergy']])\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.35, random_state=42)\n",
        "\n",
        "# Define the model\n",
        "model = LogisticRegression(penalty='elasticnet', solver='saga', max_iter=1000 , l1_ratio=0.5)\n",
        "\n",
        "# Train the logistic regression model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_train = model.predict(X_train)\n",
        "y_pred_test = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
        "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
        "\n",
        "print(f'Training Accuracy: {train_accuracy}')\n",
        "print(f'Test Accuracy: {test_accuracy}')\n",
        "print(classification_report(y_test, y_pred_test))\n",
        "\n",
        "# Save the model\n",
        "model_filename = 'lastmodel.pkl'\n",
        "joblib.dump(model, '/content/drive/MyDrive/lastmodel.pkl')\n",
        "print(f\"Model saved to {model_filename}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "param_grid = {\n",
        "    'penalty': ['l1', 'l2'],  # Types of regularization\n",
        "    'C': [0.01, 0.1, 1, 10],           # Inverse of regularization strength\n",
        "    'l1_ratio': [0.0, 0.5, 1.0],  # Only used if penalty is 'elasticnet'\n",
        "    'solver': ['saga'],                     # 'saga' is needed for l1 or elasticnet\n",
        "    'max_iter': [1000]                      # Number of iterations\n",
        "}\n",
        "# Initialize the model\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Set up GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "# Get the best parameters\n",
        "best_params = grid_search.best_params_\n",
        "print(f'Best parameters: {best_params}')\n",
        "\n",
        "# Get the best model\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions\n",
        "y_pred_train_best = best_model.predict(X_train)\n",
        "y_pred_test_best = best_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "train_accuracy_best = accuracy_score(y_train, y_pred_train_best)\n",
        "test_accuracy_best = accuracy_score(y_test, y_pred_test_best)\n",
        "\n",
        "print(f'Training Accuracy (Best Model): {train_accuracy_best}')\n",
        "print(f'Test Accuracy (Best Model): {test_accuracy_best}')\n",
        "print(classification_report(y_test, y_pred_test_best))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRKrALQlG6LT",
        "outputId": "eb35686f-c5d8-40c5-bb9a-097030348dc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1172: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters: {'C': 0.01, 'l1_ratio': 0.0, 'max_iter': 1000, 'penalty': 'l2', 'solver': 'saga'}\n",
            "Training Accuracy (Best Model): 0.6835952658129608\n",
            "Test Accuracy (Best Model): 0.6634840569266799\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.67      0.62      0.64      5420\n",
            "        True       0.66      0.71      0.68      5682\n",
            "\n",
            "    accuracy                           0.66     11102\n",
            "   macro avg       0.66      0.66      0.66     11102\n",
            "weighted avg       0.66      0.66      0.66     11102\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM38OiDl5Jz0KbKQAqxYpAp"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}